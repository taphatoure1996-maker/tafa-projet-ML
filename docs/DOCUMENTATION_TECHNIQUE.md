# Documentation Technique

## üìã Architecture du Projet

### Vue d'Ensemble

Le projet est organis√© en modules Python ind√©pendants qui peuvent √™tre utilis√©s s√©par√©ment ou ensemble. Chaque module a une responsabilit√© sp√©cifique suivant le principe de s√©paration des pr√©occupations.

```
src/
‚îú‚îÄ‚îÄ data_preprocessing.py      # Chargement et nettoyage des donn√©es
‚îú‚îÄ‚îÄ feature_engineering.py     # Construction et transformation des features
‚îú‚îÄ‚îÄ dimension_reduction.py     # ACP, PLS et visualisations
‚îú‚îÄ‚îÄ models.py                  # Mod√®les de ML (classification et r√©gression)
‚îî‚îÄ‚îÄ evaluation.py              # M√©triques, √©valuation et interpr√©tabilit√©
```

## üîÑ Pipeline de Traitement

### 1. Chargement des Donn√©es

```python
donnees = dp.charger_toutes_donnees('.')
# Retourne: {'polices': df, 'sinistres': df, 'climat': df, 'communes': df}
```

**Particularit√©s :**
- S√©parateur `;` (format europ√©en)
- Encodage automatique d√©tect√©
- Gestion des colonnes avec espaces

### 2. Pr√©traitement

#### Polices d'Assurance

```python
df_polices = dp.nettoyer_donnees_polices(donnees['polices'])
```

**Op√©rations effectu√©es :**
- Conversion des colonnes num√©riques
- Gestion des valeurs manquantes pour conducteur secondaire
- Cr√©ation d'identifiants uniques `policy_year_id`
- Traitement des valeurs aberrantes

#### Sinistres

```python
df_sinistres = dp.nettoyer_donnees_sinistres(donnees['sinistres'])
```

**Op√©rations effectu√©es :**
- Nettoyage du format `claim_amount` ("amount= 1236" ‚Üí 1236)
- Suppression des sinistres sans montant
- Cr√©ation de l'identifiant `policy_year_id`
- Conversion en types num√©riques

#### Agr√©gation des Sinistres

```python
df_sinistres_agg = dp.agregation_sinistres_par_police(df_sinistres)
```

**R√©sultat :**
- Une ligne par police-ann√©e
- `nb_sinistres_total` : nombre total de sinistres
- `montant_total` : montant cumul√©
- `a_sinistre` : indicateur binaire (0/1)

### 3. Jointure des Bases

#### Polices + Sinistres

```python
df_base = fe.joindre_polices_sinistres(df_polices, df_sinistres_agg)
```

**Type de jointure :** LEFT JOIN (conserve toutes les polices)

**R√©sultat :**
- Polices avec sinistres : `a_sinistre=1`, montants renseign√©s
- Polices sans sinistre : `a_sinistre=0`, montants √† 0

#### Ajout des Donn√©es Climatiques

```python
df_climat_agg = fe.agregation_climat_par_dept_annee(df_climat, variables_climat)
df_finale = fe.joindre_avec_climat(df_base, df_climat_agg)
```

**Logique de jointure :**
1. Extraction du d√©partement depuis `pol_insee_code` (2 premiers chiffres)
2. Mapping `id_year` vers ann√©e num√©rique (Year 0 = 2017)
3. Agr√©gation climatique par d√©partement-ann√©e (moyenne annuelle)
4. Jointure sur `(code_dept, annee)`

### 4. Construction des Features

#### Variables D√©riv√©es

```python
df_finale = fe.creer_variables_derivees(df_finale)
```

**Variables cr√©√©es :**

| Cat√©gorie | Variables |
|-----------|-----------|
| V√©hicule | `vh_age_cat`, `vh_value_cat`, `vh_puissance_cat` |
| Conducteur | `drv_age1_cat`, `drv_experience` |
| Contrat | `pol_bonus_cat`, `a_conducteur_secondaire` |
| Ratios | `ratio_puissance_poids` |

**Cat√©gorisation :**
- Utilise `pd.cut()` pour les intervalles fixes
- Utilise `pd.qcut()` pour les quantiles

## üéØ Variables Cibles

### Fr√©quence

**Variable :** `a_sinistre` (binaire 0/1)

**Distribution typique :**
- 0 (pas de sinistre) : ~89%
- 1 (au moins un sinistre) : ~11%

**Probl√©matique :** D√©s√©quilibre de classes
**Solutions appliqu√©es :**
- Stratification lors du split train/test
- M√©triques adapt√©es (AUC-ROC plut√¥t qu'accuracy)

### Gravit√©

**Variable :** `montant_total` (continue, > 0)

**Distribution :**
- Asym√©trique (distribution lognormale)
- Pr√©sence de valeurs extr√™mes
- Moyenne : ~1100‚Ç¨, M√©diane : ~600‚Ç¨

**Transformation possible :** `log(montant_total + 1)`

## üî¨ R√©duction de Dimension

### ACP (Analyse en Composantes Principales)

```python
resultats_acp = dr.analyse_acp(X_climat_scaled, n_components=20)
```

**Processus :**
1. Standardisation (mean=0, std=1)
2. Calcul des composantes principales
3. Tri par variance expliqu√©e
4. Analyse des loadings

**Interpr√©tation des composantes :**
- PC1 : Souvent li√©e √† la temp√©rature moyenne
- PC2 : Souvent li√©e aux pr√©cipitations
- PC3+ : Facteurs plus sp√©cifiques (vent, neige, etc.)

### PLS (Partial Least Squares)

```python
resultats_pls = dr.analyser_pls(X_climat_scaled, y, n_components=10)
```

**Avantage sur l'ACP :**
- Maximise la covariance avec la variable cible
- Composantes directement pr√©dictives
- Meilleure pour la mod√©lisation

## ü§ñ Mod√®les Impl√©ment√©s

### Classification (Fr√©quence)

| Mod√®le | Fonction | Hyperparam√®tres Cl√©s |
|--------|----------|---------------------|
| Logistic Regression | `entrainer_logistic_regression()` | `max_iter=1000` |
| Lasso (L1) | `entrainer_logistic_penalisee()` | `penalty='l1', C=0.1` |
| Ridge (L2) | `entrainer_logistic_penalisee()` | `penalty='l2', C=1.0` |
| ElasticNet | `entrainer_logistic_penalisee()` | `penalty='elasticnet', l1_ratio=0.5` |
| Random Forest | `entrainer_random_forest_classifier()` | `n_estimators=100, max_depth=10` |
| XGBoost | `entrainer_xgboost_classifier()` | `n_estimators=100, learning_rate=0.1` |

### R√©gression (Gravit√©)

| Mod√®le | Fonction | Hyperparam√®tres Cl√©s |
|--------|----------|---------------------|
| Linear Regression | `entrainer_regression_lineaire()` | `alpha=0.01` |
| Ridge | `entrainer_regression_penalisee()` | `method='ridge', alpha=1.0` |
| Lasso | `entrainer_regression_penalisee()` | `method='lasso', alpha=1.0` |
| Random Forest | `entrainer_random_forest_regressor()` | `n_estimators=100, max_depth=10` |
| XGBoost | `entrainer_xgboost_regressor()` | `n_estimators=100, learning_rate=0.1` |

## üìä M√©triques d'√âvaluation

### Classification

| M√©trique | Description | Interpr√©tation |
|----------|-------------|----------------|
| Accuracy | Proportion de bonnes pr√©dictions | Biais√© si classes d√©s√©quilibr√©es |
| Precision | VP / (VP + FP) | Fiabilit√© des pr√©dictions positives |
| Recall | VP / (VP + FN) | Capacit√© √† d√©tecter les sinistres |
| F1-Score | Moyenne harmonique Precision/Recall | √âquilibre entre les deux |
| **AUC-ROC** | Aire sous la courbe ROC | **M√©trique principale** (non biais√©e) |

### R√©gression

| M√©trique | Description | Unit√© | Pr√©f√©rence |
|----------|-------------|-------|------------|
| MSE | Mean Squared Error | ‚Ç¨¬≤ | P√©nalise fortement les erreurs |
| **RMSE** | Root MSE | ‚Ç¨ | **M√©trique principale** |
| MAE | Mean Absolute Error | ‚Ç¨ | Robuste aux outliers |
| R¬≤ | Coefficient de d√©termination | 0-1 | Variance expliqu√©e |
| MAPE | Mean Absolute % Error | % | Erreur relative |

## üîç Interpr√©tabilit√©

### Feature Importance

**M√©thodes impl√©ment√©es :**

1. **Coefficients lin√©aires** (Logistic/Linear Regression)
   - Valeurs directes des coefficients
   - Signe indique la direction de l'effet

2. **Importances Gini** (Random Forest, XGBoost)
   - Bas√©es sur les r√©ductions d'impuret√©
   - Normalis√©es pour sommer √† 1

3. **SHAP Values** (tous mod√®les)
   - Valeurs de Shapley
   - Contribution de chaque feature par pr√©diction
   - Visualisation avec summary plots

## ‚öôÔ∏è Optimisation des Hyperparam√®tres

### GridSearchCV

```python
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15],
    'learning_rate': [0.01, 0.1, 0.3]
}

grid_search = md.optimiser_hyperparametres(
    model, param_grid, X_train, y_train, cv=5
)
```

### Validation Crois√©e

```python
resultats_cv = md.validation_croisee(model, X, y, cv=5, scoring='roc_auc')
```

**Strat√©gies :**
- 5-fold pour rapidit√©
- 10-fold pour stabilit√©
- Stratification pour classification

## üöÄ Performance et Optimisation

### Temps d'Ex√©cution Typiques

| Op√©ration | Dur√©e |
|-----------|-------|
| Chargement donn√©es | 5-10s |
| Pr√©traitement | 10-15s |
| Jointures | 5-10s |
| ACP (50 variables) | 2-5s |
| Random Forest (100 arbres) | 30-60s |
| XGBoost (100 arbres) | 15-30s |

### Conseils d'Optimisation

1. **Donn√©es volumineuses :**
   - √âchantillonner pour les tests
   - Utiliser `n_jobs=-1` (parall√©lisation)

2. **Mod√®les lents :**
   - R√©duire `n_estimators`
   - R√©duire `max_depth`
   - Utiliser `early_stopping_rounds` (XGBoost)

3. **M√©moire limit√©e :**
   - Charger les donn√©es par chunks
   - Utiliser types de donn√©es optimis√©s (`category`, `float32`)

## üìù Bonnes Pratiques

### Code

- Documentation en fran√ßais (docstrings)
- Type hints pour les param√®tres
- Gestion des erreurs avec messages explicites
- Logging des √©tapes principales

### Mod√©lisation

- Toujours s√©parer train/test **avant** toute transformation
- Standardiser les features num√©riques
- G√©rer les valeurs manquantes explicitement
- Valider sur plusieurs m√©triques

### Reproductibilit√©

- `random_state=42` partout
- Sauvegarder les mod√®les (pickle/joblib)
- Documenter les versions des packages
- Sauvegarder les r√©sultats interm√©diaires

## üîó D√©pendances Cl√©s

| Package | Version | Usage |
|---------|---------|-------|
| pandas | ‚â•1.5.0 | Manipulation de donn√©es |
| numpy | ‚â•1.23.0 | Calculs num√©riques |
| scikit-learn | ‚â•1.2.0 | ML classique |
| xgboost | ‚â•1.7.0 | Gradient boosting |
| matplotlib | ‚â•3.6.0 | Visualisation |
| seaborn | ‚â•0.12.0 | Visualisation statistique |
| shap | ‚â•0.41.0 | Interpr√©tabilit√© |
| statsmodels | ‚â•0.13.0 | Mod√®les statistiques |

## üìö R√©f√©rences

### Machine Learning

- Hastie, T., et al. (2009). *The Elements of Statistical Learning*
- James, G., et al. (2013). *An Introduction to Statistical Learning*

### Assurance

- Denuit, M., et al. (2007). *Actuarial Modelling of Claim Counts*
- Ohlsson, E., & Johansson, B. (2010). *Non-Life Insurance Pricing with GLM*

### Python & ML

- [Scikit-learn Documentation](https://scikit-learn.org/)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [SHAP Documentation](https://shap.readthedocs.io/)
