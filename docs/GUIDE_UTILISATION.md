# Guide d'Utilisation du Projet

## üöÄ D√©marrage Rapide

### Installation

```bash
# 1. Cloner le repository
git clone https://github.com/taphatoure1996-maker/tafa-projet-ML.git
cd tafa-projet-ML

# 2. Cr√©er un environnement virtuel (recommand√©)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# 3. Installer les d√©pendances
pip install -r requirements.txt
```

## üìù Trois Fa√ßons d'Utiliser le Projet

### 1. Ex√©cution Rapide - Script de Test

Pour v√©rifier que tout fonctionne correctement :

```bash
python test_projet.py
```

Ce script teste :
- Le chargement des donn√©es
- Le pr√©traitement
- La construction des features
- Un mod√®le simple de classification

**Dur√©e : ~30 secondes**

### 2. Exemple Simple - Script d'Analyse

Pour une analyse rapide et compl√®te :

```bash
python exemple_utilisation.py
```

Ce script r√©alise :
- Analyse descriptive de la sinistralit√©
- Entra√Ænement de plusieurs mod√®les
- Comparaison des performances
- Identification des features importantes

**Dur√©e : ~1-2 minutes**

### 3. Analyse Compl√®te - Notebook Jupyter

Pour l'analyse compl√®te avec visualisations :

```bash
jupyter notebook notebooks/projet_sinistralite_climat.ipynb
```

Le notebook contient :
- Toutes les analyses descriptives
- R√©duction de dimension (ACP/PLS)
- 8+ mod√®les de machine learning
- Visualisations interactives
- Interpr√©tabilit√© (SHAP values)

**Dur√©e : 15-30 minutes**

## üìä Structure des Donn√©es

### Fichiers Requis

Les fichiers CSV suivants doivent √™tre dans le r√©pertoire racine :

- `pg17trainpol.csv` - Polices d'assurance (~100k lignes)
- `pg17trainclaim.csv` - Sinistres (~14k lignes)
- `DataClimatiques.csv` - Donn√©es m√©t√©o (~28k lignes)
- `fremuni17.csv` - Communes fran√ßaises (~50k lignes)

### Format des Donn√©es

**Important :** Les fichiers CSV utilisent le s√©parateur `;` (format europ√©en).

## üîß Utilisation Programmatique

### Exemple Minimal

```python
from src import data_preprocessing as dp
from src import feature_engineering as fe
from src import models as md

# Charger les donn√©es
donnees = dp.charger_toutes_donnees('.')

# Pr√©parer
df_polices = dp.nettoyer_donnees_polices(donnees['polices'])
df_sinistres = dp.nettoyer_donnees_sinistres(donnees['sinistres'])
df_sinistres_agg = dp.agregation_sinistres_par_police(df_sinistres)

# Joindre
df_base = fe.joindre_polices_sinistres(df_polices, df_sinistres_agg)
df_finale = fe.creer_variables_derivees(df_base)

# Mod√©liser
features = ['pol_bonus', 'drv_age1', 'vh_age', 'vh_din', 'vh_value']
X, y, _ = fe.preparer_donnees_modelisation(df_finale, features, 'a_sinistre')

X_train, X_test, y_train, y_test = md.diviser_donnees(X, y)
model = md.entrainer_random_forest_classifier(X_train, y_train)
```

## üìà R√©sultats Attendus

### Taux de Sinistralit√©

- Taux global : ~11%
- Variation selon l'usage : 8-25%
- Variation selon l'√¢ge du v√©hicule : 3-15%

### Performances des Mod√®les

**Fr√©quence (Classification) :**
- Logistic Regression : AUC ~0.61
- Random Forest : AUC ~0.62
- XGBoost : AUC ~0.63

**Gravit√© (R√©gression) :**
- Random Forest : R¬≤ ~0.15-0.25
- XGBoost : R¬≤ ~0.20-0.30

### Features Importantes

1. √Çge du v√©hicule
2. Valeur du v√©hicule
3. Bonus-malus
4. √Çge du conducteur
5. Puissance du v√©hicule

## üõ†Ô∏è Modules Disponibles

### `src.data_preprocessing`

Fonctions pour charger et nettoyer les donn√©es :
- `charger_toutes_donnees()` - Charge tous les fichiers
- `nettoyer_donnees_polices()` - Nettoie les polices
- `nettoyer_donnees_sinistres()` - Nettoie les sinistres
- `preparer_variables_climatiques()` - Pr√©pare les variables m√©t√©o

### `src.feature_engineering`

Fonctions pour construire les features :
- `joindre_polices_sinistres()` - Joint les bases
- `creer_variables_derivees()` - Cr√©e des variables suppl√©mentaires
- `selectionner_features_modelisation()` - S√©lectionne les features
- `preparer_donnees_modelisation()` - Pr√©pare X et y

### `src.dimension_reduction`

Fonctions pour la r√©duction de dimension :
- `analyse_acp()` - Analyse en Composantes Principales
- `analyser_pls()` - Partial Least Squares
- `visualiser_variance_expliquee()` - Graphiques de variance
- `interpreter_composantes_climat()` - Interpr√©tation

### `src.models`

Fonctions pour la mod√©lisation :
- `entrainer_logistic_regression()` - R√©gression logistique
- `entrainer_random_forest_classifier()` - Random Forest
- `entrainer_xgboost_classifier()` - XGBoost
- Et versions r√©gression pour la gravit√©

### `src.evaluation`

Fonctions pour l'√©valuation :
- `evaluer_classification()` - M√©triques de classification
- `evaluer_regression()` - M√©triques de r√©gression
- `comparer_modeles()` - Compare plusieurs mod√®les
- `calculer_shap_values()` - SHAP pour interpr√©tabilit√©

## üêõ D√©pannage

### Erreur : "ModuleNotFoundError"

```bash
pip install -r requirements.txt
```

### Erreur : "FileNotFoundError" pour les CSV

V√©rifiez que les fichiers CSV sont dans le bon r√©pertoire :
```bash
ls -la *.csv
```

### Performances lentes

- R√©duisez le nombre de features climatiques
- Utilisez un sous-√©chantillon des donn√©es pour les tests
- R√©duisez `n_estimators` pour Random Forest/XGBoost

### Probl√®mes avec SHAP

```bash
pip install shap
```

Si toujours des probl√®mes, commentez les sections SHAP dans le notebook.

## üìö Documentation des Variables

Consultez le fichier `Description des variables de la base de donn√©es.docx` pour :
- Description de toutes les colonnes
- Format et unit√©s
- Valeurs possibles

## ü§ù Support

Pour toute question :
1. Consultez le README.md
2. Examinez les exemples dans `exemple_utilisation.py`
3. Ouvrez une issue sur GitHub

## üìÑ Licence

Ce projet est fourni √† des fins √©ducatives et de recherche.
